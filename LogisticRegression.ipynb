{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "Nothing much in this cell after \"data.npz\" was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "  \n",
    "data = np.load('data.npz')\n",
    "X_train = data['X_train']\n",
    "X_test = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLearn Imports and Data Preprocessing\n",
    "Importing necessary SKLearn packages and preprocessing the data (Scaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Model\n",
    "Just the normal Logistic Regression model and its scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Accuracy: 0.9699\n",
      "Default Precision: 0.5500\n",
      "Default Recall: 0.2558\n",
      "Default F1 Score: 0.3492\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()\n",
    "\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Default Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Default Precision: {precision:.4f}\")\n",
    "print(f\"Default Recall: {recall:.4f}\")\n",
    "print(f\"Default F1 Score: {f1:.4f}\")\n",
    "\n",
    "#coefficients = logistic_model.coef_\n",
    "#print(\"Coefficients:\", coefficients)\n",
    "\n",
    "# y_probs = logistic_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# thresholds = [0.3, 0.4, 0.5, 0.6]\n",
    "# for thresh in thresholds:\n",
    "#     y_pred_thresh = np.where(y_probs > thresh, 1, 0)\n",
    "#     print(f\"Threshold: {thresh}\")\n",
    "#     print(f\"Accuracy: {accuracy_score(y_test, y_pred_thresh):.4f}\")\n",
    "#     print(f\"Precision: {precision_score(y_test, y_pred_thresh):.4f}\")\n",
    "#     print(f\"Recall: {recall_score(y_test, y_pred_thresh):.4f}\")\n",
    "#     print(f\"F1 Score: {f1_score(y_test, y_pred_thresh):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model With Class Weight and Max Iterations (To Optimize for Recall)\n",
    "A Logistic Regression model optimized for Recall. There is a tradeoff in precision. (Lots of false positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Accuracy: 0.8710\n",
      "Default Precision: 0.1692\n",
      "Default Recall: 0.7907\n",
      "Default F1 Score: 0.2787\n",
      "Final custom score on test set: 0.8108\n"
     ]
    }
   ],
   "source": [
    "logistic_model = LogisticRegression(class_weight='balanced', max_iter=500)\n",
    "\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Default Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Default Precision: {precision:.4f}\")\n",
    "print(f\"Default Recall: {recall:.4f}\")\n",
    "print(f\"Default F1 Score: {f1:.4f}\")\n",
    "\n",
    "# y_probs = logistic_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# thresholds = [0.3, 0.4, 0.5, 0.6]\n",
    "# for thresh in thresholds:\n",
    "#     y_pred_thresh = np.where(y_probs > thresh, 1, 0)\n",
    "#     print(f\"Threshold: {thresh}\")\n",
    "#     print(f\"Accuracy: {accuracy_score(y_test, y_pred_thresh):.4f}\")\n",
    "#     print(f\"Precision: {precision_score(y_test, y_pred_thresh):.4f}\")\n",
    "#     print(f\"Recall: {recall_score(y_test, y_pred_thresh):.4f}\")\n",
    "#     print(f\"F1 Score: {f1_score(y_test, y_pred_thresh):.4f}\")\n",
    "\n",
    "def custom_score(y_true, y_pred):\n",
    "    recall_weight = 0.75\n",
    "    accuracy_weight = 0.25\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return recall * recall_weight + accuracy * accuracy_weight\n",
    "\n",
    "final_score = custom_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Final custom score on test set: {final_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search\n",
    "This will get the best recall score for the regularization strength (C) and the max number of iterations (max_iter) on the Logistic Regression Model below. There will be Convergence Warnings due to non-convergence.\n",
    "\n",
    "## Best Hyperparameters Based on Testing with Cross-Validation (cv=5)\n",
    "- Recall: \n",
    "    - class_weight='balanced'\n",
    "    - C=0.01\n",
    "    - max_iter=100\n",
    "- Accuracy:\n",
    "    - class_weight='None'\n",
    "    - C=0.01\n",
    "    - max_iter=100\n",
    "- F1:\n",
    "    - class_weight='None'\n",
    "    - C=10\n",
    "    - max_iter=500\n",
    "- Precision:\n",
    "    - class_weight='None'\n",
    "    - C=0.01\n",
    "    - max_iter=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = {'C': [0.01, 0.1, 1, 10], 'max_iter': [100, 500, 1000]}\n",
    "\n",
    "# search = GridSearchCV(LogisticRegression(class_weight='balanced'), grid, scoring='recall', cv=5)\n",
    "# search.fit(X_train, y_train)\n",
    "\n",
    "# print(f\"Best parameters: {search.best_params_}\")\n",
    "# print(f\"Best recall from CV: {search.best_score_:.4f}\")\n",
    "\n",
    "# scores = ['accuracy', 'f1', 'precision']\n",
    "# for score in scores:\n",
    "#     search = GridSearchCV(LogisticRegression(), grid, scoring=score, cv=5)\n",
    "#     search.fit(X_train, y_train)\n",
    "\n",
    "#     print(f\"Best parameters: {search.best_params_}\")\n",
    "#     print(f\"Best {score} from CV: {search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Method\n",
    "This method has returned the best results so far across the board.\n",
    "\n",
    "## Best Hyperparameters for each Model\n",
    "Many calues of C, class_weight, and max_iter were tested for each model in the ensemble.\n",
    "- Recall:\n",
    "    - C=0.1\n",
    "    - class_weight='balanced'\n",
    "    - max_iter=100\n",
    "- Accuracy:\n",
    "    - C=0.1\n",
    "    - class_weight=None\n",
    "    - max_iter=100\n",
    "\n",
    "## Best Parameters in the Ensemble\n",
    "Different values of voting and weights were tried to maximize recall, precision, and accuracy. There is an inverse relationship between precision and recall because recall goes up when more positives get predicted and precision goes down because a lot of these positives are false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Accuracy: 0.9567\n",
      "Ensemble Precision: 0.3788\n",
      "Ensemble Recall: 0.5814\n",
      "Ensemble F1 Score: 0.4587\n"
     ]
    }
   ],
   "source": [
    "model_recall = LogisticRegression(C=0.1, class_weight='balanced', max_iter=100)\n",
    "\n",
    "model_accuracy = LogisticRegression(C=0.1, class_weight=None, max_iter=100)\n",
    "\n",
    "ensemble_model = VotingClassifier(estimators=[\n",
    "    ('recall_opt', model_recall),\n",
    "    ('accuracy_opt', model_accuracy)\n",
    "], voting='soft', weights=[1,1])\n",
    "\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "y_pred_ensemble = ensemble_model.predict(X_test)\n",
    "\n",
    "ensemble_accuracy = accuracy_score(y_test, y_pred_ensemble)\n",
    "ensemble_precision = precision_score(y_test, y_pred_ensemble)\n",
    "ensemble_recall = recall_score(y_test, y_pred_ensemble)\n",
    "ensemble_f1 = f1_score(y_test, y_pred_ensemble)\n",
    "\n",
    "print(f\"Ensemble Accuracy: {ensemble_accuracy:.4f}\")\n",
    "print(f\"Ensemble Precision: {ensemble_precision:.4f}\")\n",
    "print(f\"Ensemble Recall: {ensemble_recall:.4f}\")\n",
    "print(f\"Ensemble F1 Score: {ensemble_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
