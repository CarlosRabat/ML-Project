{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "Nothing much in this cell after \"data.npz\" was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "  \n",
    "data = np.load('data.npz')\n",
    "X_train = data['X_train']\n",
    "X_test = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLearn Imports and Data Preprocessing\n",
    "Importing necessary SKLearn packages and preprocessing the data (Scaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Model\n",
    "Just the normal Logistic Regression model and its scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Accuracy: 0.9699\n",
      "Default Precision: 0.5500\n",
      "Default Recall: 0.2558\n",
      "Default F1 Score: 0.3492\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      1321\n",
      "           1       0.55      0.26      0.35        43\n",
      "\n",
      "    accuracy                           0.97      1364\n",
      "   macro avg       0.76      0.62      0.67      1364\n",
      "weighted avg       0.96      0.97      0.96      1364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()\n",
    "\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Default Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Default Precision: {precision:.4f}\")\n",
    "print(f\"Default Recall: {recall:.4f}\")\n",
    "print(f\"Default F1 Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "#coefficients = logistic_model.coef_\n",
    "#print(\"Coefficients:\", coefficients)\n",
    "\n",
    "# y_probs = logistic_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# thresholds = [0.3, 0.4, 0.5, 0.6]\n",
    "# for thresh in thresholds:\n",
    "#     y_pred_thresh = np.where(y_probs > thresh, 1, 0)\n",
    "#     print(f\"Threshold: {thresh}\")\n",
    "#     print(f\"Accuracy: {accuracy_score(y_test, y_pred_thresh):.4f}\")\n",
    "#     print(f\"Precision: {precision_score(y_test, y_pred_thresh):.4f}\")\n",
    "#     print(f\"Recall: {recall_score(y_test, y_pred_thresh):.4f}\")\n",
    "#     print(f\"F1 Score: {f1_score(y_test, y_pred_thresh):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model With Class Weight and Max Iterations (To Optimize for Recall)\n",
    "A Logistic Regression model optimized for Recall. There is a tradeoff in precision. (Lots of false positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Accuracy: 0.8710\n",
      "Default Precision: 0.1692\n",
      "Default Recall: 0.7907\n",
      "Default F1 Score: 0.2787\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.87      0.93      1321\n",
      "           1       0.17      0.79      0.28        43\n",
      "\n",
      "    accuracy                           0.87      1364\n",
      "   macro avg       0.58      0.83      0.60      1364\n",
      "weighted avg       0.97      0.87      0.91      1364\n",
      "\n",
      "Final custom score on test set: 0.8108\n"
     ]
    }
   ],
   "source": [
    "logistic_model = LogisticRegression(class_weight='balanced', max_iter=500)\n",
    "\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Default Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Default Precision: {precision:.4f}\")\n",
    "print(f\"Default Recall: {recall:.4f}\")\n",
    "print(f\"Default F1 Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# y_probs = logistic_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# thresholds = [0.3, 0.4, 0.5, 0.6]\n",
    "# for thresh in thresholds:\n",
    "#     y_pred_thresh = np.where(y_probs > thresh, 1, 0)\n",
    "#     print(f\"Threshold: {thresh}\")\n",
    "#     print(f\"Accuracy: {accuracy_score(y_test, y_pred_thresh):.4f}\")\n",
    "#     print(f\"Precision: {precision_score(y_test, y_pred_thresh):.4f}\")\n",
    "#     print(f\"Recall: {recall_score(y_test, y_pred_thresh):.4f}\")\n",
    "#     print(f\"F1 Score: {f1_score(y_test, y_pred_thresh):.4f}\")\n",
    "\n",
    "def custom_score(y_true, y_pred):\n",
    "    recall_weight = 0.75\n",
    "    accuracy_weight = 0.25\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return recall * recall_weight + accuracy * accuracy_weight\n",
    "\n",
    "final_score = custom_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Final custom score on test set: {final_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search\n",
    "This will get the best recall score for the regularization strength (C) and the max number of iterations (max_iter) on the Logistic Regression Model below. There will be Convergence Warnings due to non-convergence.\n",
    "\n",
    "## Best Hyperparameters Based on Testing with Cross-Validation (cv=5)\n",
    "- Recall: \n",
    "    - class_weight='balanced'\n",
    "    - C=0.01\n",
    "    - max_iter=100\n",
    "- Accuracy:\n",
    "    - class_weight='None'\n",
    "    - C=0.01\n",
    "    - max_iter=100\n",
    "- F1:\n",
    "    - class_weight='None'\n",
    "    - C=10\n",
    "    - max_iter=500\n",
    "- Precision:\n",
    "    - class_weight='None'\n",
    "    - C=0.01\n",
    "    - max_iter=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = {'C': [0.01, 0.1, 1, 10], 'max_iter': [100, 500, 1000]}\n",
    "\n",
    "# search = GridSearchCV(LogisticRegression(class_weight='balanced'), grid, scoring='recall', cv=5)\n",
    "# search.fit(X_train, y_train)\n",
    "\n",
    "# print(f\"Best parameters: {search.best_params_}\")\n",
    "# print(f\"Best recall from CV: {search.best_score_:.4f}\")\n",
    "\n",
    "# scores = ['accuracy', 'f1', 'precision']\n",
    "# for score in scores:\n",
    "#     search = GridSearchCV(LogisticRegression(), grid, scoring=score, cv=5)\n",
    "#     search.fit(X_train, y_train)\n",
    "\n",
    "#     print(f\"Best parameters: {search.best_params_}\")\n",
    "#     print(f\"Best {score} from CV: {search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Method\n",
    "This method has returned the best results so far across the board.\n",
    "\n",
    "## Best Hyperparameters for each Model\n",
    "Many calues of C, class_weight, and max_iter were tested for each model in the ensemble.\n",
    "- Recall:\n",
    "    - C=0.1\n",
    "    - class_weight='balanced'\n",
    "    - max_iter=100\n",
    "- Accuracy:\n",
    "    - C=0.1\n",
    "    - class_weight=None\n",
    "    - max_iter=100\n",
    "\n",
    "## Best Parameters in the Ensemble\n",
    "Different values of voting and weights were tried to maximize recall, precision, and accuracy. There is an inverse relationship between precision and recall because recall goes up when more positives get predicted and precision goes down because a lot of these positives are false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Accuracy: 0.9567\n",
      "Ensemble Precision: 0.3788\n",
      "Ensemble Recall: 0.5814\n",
      "Ensemble F1 Score: 0.4587\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.87      0.93      1321\n",
      "           1       0.17      0.79      0.28        43\n",
      "\n",
      "    accuracy                           0.87      1364\n",
      "   macro avg       0.58      0.83      0.60      1364\n",
      "weighted avg       0.97      0.87      0.91      1364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_recall = LogisticRegression(C=0.1, class_weight='balanced', max_iter=100)\n",
    "\n",
    "model_accuracy = LogisticRegression(C=0.1, class_weight=None, max_iter=100)\n",
    "\n",
    "ensemble_model = VotingClassifier(estimators=[\n",
    "    ('recall_opt', model_recall),\n",
    "    ('accuracy_opt', model_accuracy)\n",
    "], voting='soft', weights=[1,1])\n",
    "\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "y_pred_ensemble = ensemble_model.predict(X_test)\n",
    "\n",
    "ensemble_accuracy = accuracy_score(y_test, y_pred_ensemble)\n",
    "ensemble_precision = precision_score(y_test, y_pred_ensemble)\n",
    "ensemble_recall = recall_score(y_test, y_pred_ensemble)\n",
    "ensemble_f1 = f1_score(y_test, y_pred_ensemble)\n",
    "\n",
    "print(f\"Ensemble Accuracy: {ensemble_accuracy:.4f}\")\n",
    "print(f\"Ensemble Precision: {ensemble_precision:.4f}\")\n",
    "print(f\"Ensemble Recall: {ensemble_recall:.4f}\")\n",
    "print(f\"Ensemble F1 Score: {ensemble_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most Important Features\n",
    "This will print out the most significant features (based on their weights in 100 randomly shuffled LogisticRegression models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('data.npz')\n",
    "X_train = data['X_train']\n",
    "X_test = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_test = data['y_test']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()\n",
    "\n",
    "num_models = 5\n",
    "feature_per_model = 19\n",
    "total_features = X_train.shape[1]\n",
    "assert total_features == 95, \"The total number of features is not 95 as expected.\"\n",
    "\n",
    "indices = np.arange(total_features)\n",
    "\n",
    "important_features_dict = {}\n",
    "model_results = {}\n",
    "final_features = {index: {'count': 0} for index in indices}\n",
    "\n",
    "for run in range(0, 100):\n",
    "    np.random.shuffle(indices)\n",
    "    feature_groups = np.array_split(indices, num_models)\n",
    "    important_features = [] \n",
    "\n",
    "    for i, features in enumerate(feature_groups, start=1):\n",
    "        X_train_subset = X_train[:, features]\n",
    "        X_test_subset = X_test[:, features]\n",
    "\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_train_subset, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test_subset)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        model_results[f'Model_{run}.{i}'] = {\n",
    "            'Features': features,\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1 Score': f1,\n",
    "            'Coefficients': model.coef_\n",
    "        }\n",
    "\n",
    "        # print(f\"Model {run}.{i} Results:\")\n",
    "        # print(f\"Features: {features}\")\n",
    "        # print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        # print(f\"Precision: {precision:.4f}\")\n",
    "        # print(f\"Recall: {recall:.4f}\")\n",
    "        # print(f\"F1 Score: {f1:.4f}\")\n",
    "        # print(f\"Coefficients: {model.coef_}\\n\")\n",
    "        for x in range(len(model.coef_[0])):\n",
    "            if model.coef_[0][x] > 0.5 or model.coef_[0][x] < -0.5:\n",
    "                #print(f\"Feature index with high/low weight: {features[x]}\")\n",
    "                important_features.append(features[x])\n",
    "                final_features[features[x]]['count'] += 1\n",
    "\n",
    "    important_features_dict[f'Run_{run}'] = {'Features': important_features}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Different Logistic Regression Models on Different Number of Features\n",
    "feature_data['count] is basically the relevancy score a feature has in making a classification. The higher the count, the more often you will find that feature having a high weight in a logistic regression model that is being trained on a subset of the full feature set. This means that features with count equal to 100 will have a high weight in all 100 model iterations that this feature can be found in. Ultimately, this count is meaningless because the logistic regression models trained on all features have the best performance on the test set. This information will be used in our other models on this bankruptcy data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Logistic Regression model trained on selected features with significance count of 0 or more:\n",
      "Accuracy: 0.9699\n",
      "Precision: 0.5500\n",
      "Recall: 0.2558\n",
      "F1 Score: 0.3492\n",
      "Results for Logistic Regression model trained on selected features with significance count of 5 or more:\n",
      "Accuracy: 0.9670\n",
      "Precision: 0.4500\n",
      "Recall: 0.2093\n",
      "F1 Score: 0.2857\n",
      "Results for Logistic Regression model trained on selected features with significance count of 10 or more:\n",
      "Accuracy: 0.9655\n",
      "Precision: 0.4000\n",
      "Recall: 0.1860\n",
      "F1 Score: 0.2540\n",
      "Results for Logistic Regression model trained on selected features with significance count of 20 or more:\n",
      "Accuracy: 0.9677\n",
      "Precision: 0.4706\n",
      "Recall: 0.1860\n",
      "F1 Score: 0.2667\n",
      "Results for Logistic Regression model trained on selected features with significance count of 30 or more:\n",
      "Accuracy: 0.9677\n",
      "Precision: 0.4706\n",
      "Recall: 0.1860\n",
      "F1 Score: 0.2667\n",
      "Results for Logistic Regression model trained on selected features with significance count of 40 or more:\n",
      "Accuracy: 0.9663\n",
      "Precision: 0.4211\n",
      "Recall: 0.1860\n",
      "F1 Score: 0.2581\n",
      "Results for Logistic Regression model trained on selected features with significance count of 50 or more:\n",
      "Accuracy: 0.9655\n",
      "Precision: 0.4000\n",
      "Recall: 0.1860\n",
      "F1 Score: 0.2540\n",
      "Results for Logistic Regression model trained on selected features with significance count of 60 or more:\n",
      "Accuracy: 0.9633\n",
      "Precision: 0.3333\n",
      "Recall: 0.1628\n",
      "F1 Score: 0.2188\n",
      "Results for Logistic Regression model trained on selected features with significance count of 70 or more:\n",
      "Accuracy: 0.9641\n",
      "Precision: 0.3500\n",
      "Recall: 0.1628\n",
      "F1 Score: 0.2222\n",
      "Results for Logistic Regression model trained on selected features with significance count of 80 or more:\n",
      "Accuracy: 0.9641\n",
      "Precision: 0.3500\n",
      "Recall: 0.1628\n",
      "F1 Score: 0.2222\n",
      "Results for Logistic Regression model trained on selected features with significance count of 90 or more:\n",
      "Accuracy: 0.9663\n",
      "Precision: 0.4118\n",
      "Recall: 0.1628\n",
      "F1 Score: 0.2333\n",
      "Results for Logistic Regression model trained on selected features with significance count of 95 or more:\n",
      "Accuracy: 0.9692\n",
      "Precision: 0.5333\n",
      "Recall: 0.1860\n",
      "F1 Score: 0.2759\n",
      "Results for Logistic Regression model trained on selected features with significance count of 99 or more:\n",
      "Accuracy: 0.9677\n",
      "Precision: 0.4706\n",
      "Recall: 0.1860\n",
      "F1 Score: 0.2667\n"
     ]
    }
   ],
   "source": [
    "threshold = [0, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 95, 99]\n",
    "#print(final_features) # type: ignore\n",
    "\n",
    "for thresh in threshold:\n",
    "    new_feature_set = []\n",
    "    for feature_index, feature_data in final_features.items():\n",
    "        if feature_data['count'] >= thresh:\n",
    "            #print(f\"Feature index {feature_index} has a significant count of {feature_data['count']}\")\n",
    "            new_feature_set.append(feature_index)\n",
    "\n",
    "    X_train_selected = X_train[:, new_feature_set]\n",
    "    X_test_selected = X_test[:, new_feature_set]\n",
    "\n",
    "    model_selected = LogisticRegression()\n",
    "    model_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred_selected = model_selected.predict(X_test_selected)\n",
    "\n",
    "    # Calculate and print the evaluation metrics\n",
    "    accuracy_selected = accuracy_score(y_test, y_pred_selected)\n",
    "    precision_selected = precision_score(y_test, y_pred_selected)\n",
    "    recall_selected = recall_score(y_test, y_pred_selected)\n",
    "    f1_selected = f1_score(y_test, y_pred_selected)\n",
    "\n",
    "    print(f\"Results for Logistic Regression model trained on selected features with significance count of {thresh} or more:\")\n",
    "    print(f\"Accuracy: {accuracy_selected:.4f}\")\n",
    "    print(f\"Precision: {precision_selected:.4f}\")\n",
    "    print(f\"Recall: {recall_selected:.4f}\")\n",
    "    print(f\"F1 Score: {f1_selected:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
