{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "Nothing much here. Just importing the repo and putting into X and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "  \n",
    "data = np.load('data.npz')\n",
    "X_train = data['X_train']\n",
    "X_test = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLearn Imports and Data Preprocessing\n",
    "Importing necessary SKLearn packages and preprocessing the data. (80/20 train test split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Decision Tree Classifier\n",
    "When reading a paper on Bankruptcy Prediction, Random Forest and XGBoost were used. Upon the creation of this ipynb, Decision Trees, Random Forest, and XGBoost were all tested and the default XGBoost and Decision Tree returned simliar scores, so the Decision Tree algorithm was chosen as the first path of exploration with hopes of being able to have more optimization with different hyperparameters and custom ensembling in the DT algorithm. These were the scores of each default algorithm:\n",
    "- Scaled:\n",
    "    - Decision Tree:\n",
    "        - Accuracy: 0.9589\n",
    "        - Precision: 0.3860\n",
    "        - Recall: 0.5116\n",
    "        - F1: 0.4400\n",
    "    - Random Forest:\n",
    "        - Accuracy: 0.9692\n",
    "        - Precision: 0.5294\n",
    "        - Recall: 0.2093\n",
    "        - F1: 0.3000\n",
    "    - XGBoost:\n",
    "        - Accuracy: 0.9721\n",
    "        - Precision: 0.6087\n",
    "        - Recall: 0.3256\n",
    "        - F1: 0.4242\n",
    "- Not Scaled:\n",
    "    - Decision Tree:\n",
    "        - Accuracy: 0.9494\n",
    "        - Precision: 0.2759\n",
    "        - Recall: 0.3721\n",
    "        - F1: 0.3168\n",
    "    - Random Forest:\n",
    "        - Accuracy: 0.9699\n",
    "        - Precision: 0.5625\n",
    "        - Recall: 0.2093\n",
    "        - F1: 0.3051\n",
    "    - XGBoost:\n",
    "        - Accuracy: 0.9736\n",
    "        - Precision: 0.7059\n",
    "        - Recall: 0.2791\n",
    "        - F1: 0.4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.9589\n",
      "Decision Tree Precision: 0.3860\n",
      "Decision Tree Recall: 0.5116\n",
      "Decision Tree F1 Score: 0.4400\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98      1321\n",
      "           1       0.39      0.51      0.44        43\n",
      "\n",
      "    accuracy                           0.96      1364\n",
      "   macro avg       0.68      0.74      0.71      1364\n",
      "weighted avg       0.97      0.96      0.96      1364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()\n",
    "\n",
    "dt_model = DecisionTreeClassifier(random_state=69)\n",
    "\n",
    "dt_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "dt_y_pred = dt_model.predict(X_test_scaled)\n",
    "\n",
    "dt_accuracy = accuracy_score(y_test, dt_y_pred)\n",
    "dt_precision = precision_score(y_test, dt_y_pred)\n",
    "dt_recall = recall_score(y_test, dt_y_pred)\n",
    "dt_f1 = f1_score(y_test, dt_y_pred)\n",
    "\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy:.4f}\")\n",
    "print(f\"Decision Tree Precision: {dt_precision:.4f}\")\n",
    "print(f\"Decision Tree Recall: {dt_recall:.4f}\")\n",
    "print(f\"Decision Tree F1 Score: {dt_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, dt_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9699\n",
      "Random Forest Precision: 0.5625\n",
      "Random Forest Recall: 0.2093\n",
      "Random Forest F1 Score: 0.3051\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      1321\n",
      "           1       0.56      0.21      0.31        43\n",
      "\n",
      "    accuracy                           0.97      1364\n",
      "   macro avg       0.77      0.60      0.64      1364\n",
      "weighted avg       0.96      0.97      0.96      1364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=69)\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "rf_y_pred = rf_model.predict(X_test)\n",
    "\n",
    "rf_accuracy = accuracy_score(y_test, rf_y_pred)\n",
    "rf_precision = precision_score(y_test, rf_y_pred)\n",
    "rf_recall = recall_score(y_test, rf_y_pred)\n",
    "rf_f1 = f1_score(y_test, rf_y_pred)\n",
    "\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
    "print(f\"Random Forest Precision: {rf_precision:.4f}\")\n",
    "print(f\"Random Forest Recall: {rf_recall:.4f}\")\n",
    "print(f\"Random Forest F1 Score: {rf_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, rf_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.9736\n",
      "XGBoost Precision: 0.7059\n",
      "XGBoost Recall: 0.2791\n",
      "XGBoost F1 Score: 0.4000\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      1321\n",
      "           1       0.71      0.28      0.40        43\n",
      "\n",
      "    accuracy                           0.97      1364\n",
      "   macro avg       0.84      0.64      0.69      1364\n",
      "weighted avg       0.97      0.97      0.97      1364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=69)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_y_pred)\n",
    "xgb_precision = precision_score(y_test, xgb_y_pred)\n",
    "xgb_recall = recall_score(y_test, xgb_y_pred)\n",
    "xgb_f1 = f1_score(y_test, xgb_y_pred)\n",
    "\n",
    "print(f\"XGBoost Accuracy: {xgb_accuracy:.4f}\")\n",
    "print(f\"XGBoost Precision: {xgb_precision:.4f}\")\n",
    "print(f\"XGBoost Recall: {xgb_recall:.4f}\")\n",
    "print(f\"XGBoost F1 Score: {xgb_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, xgb_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search\n",
    "This will get the best overall model for the parameters max_depth, min_samples_split, min_samples_leaf, and criterion. (72 models in total)\n",
    "\n",
    "## Best Hyperparameters Based on Testing with Cross-Validation (cv=5)\n",
    "- Criterion='gini'\n",
    "- max_depth=None\n",
    "- min_samples_leaf=5\n",
    "- min_samples_split=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'max_depth': [None, 10, 20, 30],\n",
    "#     'min_samples_split': [2, 10, 20],\n",
    "#     'min_samples_leaf': [1, 5, 10],\n",
    "#     'criterion': ['gini', 'entropy']\n",
    "# }\n",
    "\n",
    "# grid_search = GridSearchCV(dt_model, param_grid, cv=5, scoring='f1', verbose=1)\n",
    "\n",
    "# best_model = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# dt_y_pred = best_model.predict(X_test)\n",
    "\n",
    "# dt_accuracy = accuracy_score(y_test, dt_y_pred)\n",
    "# dt_precision = precision_score(y_test, dt_y_pred)\n",
    "# dt_recall = recall_score(y_test, dt_y_pred)\n",
    "# dt_f1 = f1_score(y_test, dt_y_pred)\n",
    "\n",
    "# print(f\"Best Decision Tree Parameters: {best_model.best_params_}\")\n",
    "# print(f\"Decision Tree Accuracy: {dt_accuracy:.4f}\")\n",
    "# print(f\"Decision Tree Precision: {dt_precision:.4f}\")\n",
    "# print(f\"Decision Tree Recall: {dt_recall:.4f}\")\n",
    "# print(f\"Decision Tree F1 Score: {dt_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Testing\n",
    "After further testing, these hyperparameters are the best for the Decision Tree. (min_samples_split=3 is the only parameter different from default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.9597\n",
      "Decision Tree Precision: 0.3966\n",
      "Decision Tree Recall: 0.5349\n",
      "Decision Tree F1 Score: 0.4554\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98      1321\n",
      "           1       0.40      0.53      0.46        43\n",
      "\n",
      "    accuracy                           0.96      1364\n",
      "   macro avg       0.69      0.75      0.72      1364\n",
      "weighted avg       0.97      0.96      0.96      1364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_model = DecisionTreeClassifier(random_state=69, max_depth=None, criterion='gini', min_samples_split=3, min_samples_leaf=1)\n",
    "\n",
    "dt_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "dt_y_pred = dt_model.predict(X_test_scaled)\n",
    "\n",
    "dt_accuracy = accuracy_score(y_test, dt_y_pred)\n",
    "dt_precision = precision_score(y_test, dt_y_pred)\n",
    "dt_recall = recall_score(y_test, dt_y_pred)\n",
    "dt_f1 = f1_score(y_test, dt_y_pred)\n",
    "\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy:.4f}\")\n",
    "print(f\"Decision Tree Precision: {dt_precision:.4f}\")\n",
    "print(f\"Decision Tree Recall: {dt_recall:.4f}\")\n",
    "print(f\"Decision Tree F1 Score: {dt_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, dt_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search\n",
    "This will get the best overall model for the parameters max_depth, min_child_weight, gamma, subsample, and colsample_bytree. (405 models in total)\n",
    "\n",
    "## Best Hyperparameters Based on Testing with Cross-Validation (cv=5)\n",
    "- ROC_AUC (F1 score of 0.44)\n",
    "    - max_depth=None\n",
    "    - min_child_weight=3\n",
    "    - gamma=0.1\n",
    "    - subsample=0.8\n",
    "    - colsample_bytree=0.9\n",
    "- F1 (Somehow the F1 score was lower than the model for ROC_AUC) (0.4375)\n",
    "    - max_depth=2\n",
    "    - min_child_weight=3\n",
    "    - gamma=0.0\n",
    "    - subsample=0.7\n",
    "    - colsample_bytree=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_test = {\n",
    "#     'max_depth': [None, 2, 3, 5, 10],\n",
    "#     'min_child_weight': [1, 3, 5],\n",
    "#     'gamma': [0.0, 0.1, 0.2],\n",
    "#     'subsample': [0.7, 0.8, 0.9],\n",
    "#     'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "# }\n",
    "\n",
    "# gsearch = GridSearchCV(estimator=xgb.XGBClassifier(learning_rate=0.1, n_estimators=140, objective='binary:logistic',\n",
    "#                                                     silent=True, nthread=1),\n",
    "#                        param_grid=param_test, scoring='f1', n_jobs=4, cv=5)\n",
    "\n",
    "# best_model = gsearch.fit(X_train, y_train)\n",
    "\n",
    "# xg_y_pred = best_model.predict(X_test)\n",
    "\n",
    "# xg_accuracy = accuracy_score(y_test, xg_y_pred)\n",
    "# xg_precision = precision_score(y_test, xg_y_pred)\n",
    "# xg_recall = recall_score(y_test, xg_y_pred)\n",
    "# xg_f1 = f1_score(y_test, xg_y_pred)\n",
    "\n",
    "# print(f\"Best Decision Tree Parameters: {best_model.best_params_}\")\n",
    "# print(f\"XGB Accuracy: {xg_accuracy:.4f}\")\n",
    "# print(f\"XGB Precision: {xg_precision:.4f}\")\n",
    "# print(f\"XGB Recall: {xg_recall:.4f}\")\n",
    "# print(f\"XGB F1 Score: {xg_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
