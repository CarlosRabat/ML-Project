{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "Nothing much here. Just importing the repo and putting into X and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "  \n",
    "data = np.load('data.npz')\n",
    "X_train = data['X_train']\n",
    "X_test = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLearn Imports and Data Preprocessing\n",
    "Importing necessary SKLearn packages and preprocessing the data. (80/20 train test split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Decision Tree Classifier\n",
    "When reading a paper on Bankruptcy Prediction, Random Forest and XGBoost were used. Upon the creation of this ipynb, Decision Trees, Random Forest, and XGBoost were all tested and the default XGBoost and Decision Tree returned simliar scores, so the Decision Tree algorithm was chosen as the first path of exploration with hopes of being able to have more optimization with different hyperparameters and custom ensembling in the DT algorithm. These were the scores of each default algorithm:\n",
    "- Decision Tree:\n",
    "    - Accuracy: 0.9589\n",
    "    - Precision: 0.3860\n",
    "    - Recall: 0.5116\n",
    "    - F1: 0.4400\n",
    "- Random Forest:\n",
    "    - Accuracy: 0.9692\n",
    "    - Precision: 0.5294\n",
    "    - Recall: 0.2093\n",
    "    - F1: 0.3000\n",
    "- XGBoost:\n",
    "    - Accuracy: 0.9721\n",
    "    - Precision: 0.6087\n",
    "    - Recall: 0.3256\n",
    "    - F1: 0.4242"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.9589\n",
      "Decision Tree Precision: 0.3860\n",
      "Decision Tree Recall: 0.5116\n",
      "Decision Tree F1 Score: 0.4400\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()\n",
    "\n",
    "dt_model = DecisionTreeClassifier(random_state=69)\n",
    "\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "dt_y_pred = dt_model.predict(X_test)\n",
    "\n",
    "dt_accuracy = accuracy_score(y_test, dt_y_pred)\n",
    "dt_precision = precision_score(y_test, dt_y_pred)\n",
    "dt_recall = recall_score(y_test, dt_y_pred)\n",
    "dt_f1 = f1_score(y_test, dt_y_pred)\n",
    "\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy:.4f}\")\n",
    "print(f\"Decision Tree Precision: {dt_precision:.4f}\")\n",
    "print(f\"Decision Tree Recall: {dt_recall:.4f}\")\n",
    "print(f\"Decision Tree F1 Score: {dt_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9692\n",
      "Random Forest Precision: 0.5294\n",
      "Random Forest Recall: 0.2093\n",
      "Random Forest F1 Score: 0.3000\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=69)\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "rf_y_pred = rf_model.predict(X_test)\n",
    "\n",
    "rf_accuracy = accuracy_score(y_test, rf_y_pred)\n",
    "rf_precision = precision_score(y_test, rf_y_pred)\n",
    "rf_recall = recall_score(y_test, rf_y_pred)\n",
    "rf_f1 = f1_score(y_test, rf_y_pred)\n",
    "\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
    "print(f\"Random Forest Precision: {rf_precision:.4f}\")\n",
    "print(f\"Random Forest Recall: {rf_recall:.4f}\")\n",
    "print(f\"Random Forest F1 Score: {rf_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.9721\n",
      "XGBoost Precision: 0.6087\n",
      "XGBoost Recall: 0.3256\n",
      "XGBoost F1 Score: 0.4242\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=69)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_y_pred)\n",
    "xgb_precision = precision_score(y_test, xgb_y_pred)\n",
    "xgb_recall = recall_score(y_test, xgb_y_pred)\n",
    "xgb_f1 = f1_score(y_test, xgb_y_pred)\n",
    "\n",
    "print(f\"XGBoost Accuracy: {xgb_accuracy:.4f}\")\n",
    "print(f\"XGBoost Precision: {xgb_precision:.4f}\")\n",
    "print(f\"XGBoost Recall: {xgb_recall:.4f}\")\n",
    "print(f\"XGBoost F1 Score: {xgb_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search\n",
    "This will get the best overall model for the parameters max_depth, min_samples_split, min_samples_leaf, and criterion. (72 models in total)\n",
    "\n",
    "## Best Hyperparameters Based on Testing with Cross-Validation (cv=5)\n",
    "- Criterion='gini'\n",
    "- max_depth=None\n",
    "- min_samples_leaf=5\n",
    "- min_samples_split=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'max_depth': [None, 10, 20, 30],\n",
    "#     'min_samples_split': [2, 10, 20],\n",
    "#     'min_samples_leaf': [1, 5, 10],\n",
    "#     'criterion': ['gini', 'entropy']\n",
    "# }\n",
    "\n",
    "# grid_search = GridSearchCV(dt_model, param_grid, cv=5, scoring='f1', verbose=1)\n",
    "\n",
    "# best_model = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# dt_y_pred = best_model.predict(X_test)\n",
    "\n",
    "# dt_accuracy = accuracy_score(y_test, dt_y_pred)\n",
    "# dt_precision = precision_score(y_test, dt_y_pred)\n",
    "# dt_recall = recall_score(y_test, dt_y_pred)\n",
    "# dt_f1 = f1_score(y_test, dt_y_pred)\n",
    "\n",
    "# print(f\"Best Decision Tree Parameters: {best_model.best_params_}\")\n",
    "# print(f\"Decision Tree Accuracy: {dt_accuracy:.4f}\")\n",
    "# print(f\"Decision Tree Precision: {dt_precision:.4f}\")\n",
    "# print(f\"Decision Tree Recall: {dt_recall:.4f}\")\n",
    "# print(f\"Decision Tree F1 Score: {dt_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Testing\n",
    "After further testing, these hyperparameters are the best for the Decision Tree. (min_samples_split=3 is the only one different from default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.9597\n",
      "Decision Tree Precision: 0.3966\n",
      "Decision Tree Recall: 0.5349\n",
      "Decision Tree F1 Score: 0.4554\n"
     ]
    }
   ],
   "source": [
    "dt_model = DecisionTreeClassifier(random_state=69, max_depth=None, criterion='gini', min_samples_split=3, min_samples_leaf=1)\n",
    "\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "dt_y_pred = dt_model.predict(X_test)\n",
    "\n",
    "dt_accuracy = accuracy_score(y_test, dt_y_pred)\n",
    "dt_precision = precision_score(y_test, dt_y_pred)\n",
    "dt_recall = recall_score(y_test, dt_y_pred)\n",
    "dt_f1 = f1_score(y_test, dt_y_pred)\n",
    "\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy:.4f}\")\n",
    "print(f\"Decision Tree Precision: {dt_precision:.4f}\")\n",
    "print(f\"Decision Tree Recall: {dt_recall:.4f}\")\n",
    "print(f\"Decision Tree F1 Score: {dt_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
